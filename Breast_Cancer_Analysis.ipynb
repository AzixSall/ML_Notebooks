{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzixSall/ML_Notebooks/blob/main/Breast_Cancer_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0JiIYdB8hrb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras_cv\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUtHwAbraSkN"
      },
      "outputs": [],
      "source": [
        "def strong_augmentation(image):\n",
        "    image = tf.image.random_brightness(image, 0.3)\n",
        "    image = tf.image.random_contrast(image, 0.7, 1.3)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sazJbDLhzho9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "IMG_SIZE = 200\n",
        "CHANNELS = 1\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "    feature_description = {\n",
        "        'image': tf.io.FixedLenFeature([], tf.string),\n",
        "        'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'label': tf.io.FixedLenFeature([], tf.int64)\n",
        "    }\n",
        "\n",
        "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
        "    image = tf.reshape(image, [299, 299, 1])\n",
        "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    label = parsed_features['label']\n",
        "\n",
        "#    label = tf.where(\n",
        "#        tf.equal(parsed_features['label_normal'], tf.constant(0, dtype=tf.int64)),\n",
        "#        tf.constant(0, dtype=tf.int64),\n",
        "#        parsed_features['label']\n",
        "#    )\n",
        "\n",
        "#    image = tf.cond(\n",
        "#        tf.not_equal(label, 0),\n",
        "#        lambda: strong_augmentation(image),\n",
        "#        lambda: image\n",
        "#    )\n",
        "\n",
        "    label = tf.one_hot(label, 5, dtype=tf.float32)\n",
        "\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ-xxC2x-QQD"
      },
      "outputs": [],
      "source": [
        "def create_dataset(tfrecord_files, batch_size=BATCH_SIZE):\n",
        "    \"\"\"Create dataset from TFRecord files\"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_files)\n",
        "    dataset = dataset.map(parse_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDW7LfGCzkln"
      },
      "outputs": [],
      "source": [
        "def create_numpy_dataset(data_path, labels_path, batch_size=BATCH_SIZE, is_training=False):\n",
        "    if isinstance(data_path, str):\n",
        "        data = np.load(data_path, mmap_mode='r')\n",
        "        labels = np.load(labels_path, mmap_mode='r')\n",
        "    else:\n",
        "        data = data_path\n",
        "        labels = labels_path\n",
        "\n",
        "    def generator():\n",
        "        for i in range(len(data)):\n",
        "            image = data[i].reshape(299, 299, 1)\n",
        "            image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
        "            #image = image.astype('uint8')\n",
        "            image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "            label_index = labels[i]\n",
        "            one_hot_label = np.zeros(5, dtype=np.float32)\n",
        "            one_hot_label[label_index] = 1.0\n",
        "\n",
        "            yield image, one_hot_label\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(IMG_SIZE, IMG_SIZE, 1), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(5,), dtype=tf.float32)\n",
        "        )\n",
        "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRsFQkE_-VlZ"
      },
      "outputs": [],
      "source": [
        "print(\"Loading training dataset...\")\n",
        "train_files = [f'Mammography/training10_{i}/training10_{i}.tfrecords' for i in range(5)]\n",
        "train_dataset = create_dataset(train_files, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "PPN7JiGTaSkP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def display_batch(dataset, num_images=8):\n",
        "    \"\"\"\n",
        "    Display images from a batch of the dataset\n",
        "    \"\"\"\n",
        "    # Get one batch\n",
        "    for images, labels in dataset.take(1):\n",
        "        plt.figure(figsize=(15, 8))\n",
        "\n",
        "        for i in range(min(num_images, len(images))):\n",
        "            plt.subplot(2, 4, i + 1)\n",
        "\n",
        "            img = images[i].numpy()\n",
        "            label = np.argmax(labels[i].numpy())\n",
        "\n",
        "            label_names = ['Normal', 'Abnormal 1', 'Abnormal 2',\n",
        "                         'Abnormal 3', 'Abnormal 4']\n",
        "            title = f'Label: {label_names[label]}'\n",
        "\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.title(title)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "display_batch(train_dataset)\n",
        "\n",
        "#for batch in train_dataset.take(10):\n",
        "#    display_batch(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zd_308Z7-ZU0"
      },
      "outputs": [],
      "source": [
        "print(\"Loading and combining validation/test data...\")\n",
        "test_data = np.load('Mammography/test10_data/test10_data.npy', mmap_mode='r')\n",
        "test_labels = np.load('Mammography/test10_labels.npy', mmap_mode='r')\n",
        "\n",
        "cv_data = np.load('Mammography/cv10_data/cv10_data.npy', mmap_mode='r')\n",
        "cv_labels = np.load('Mammography/cv10_labels.npy', mmap_mode='r')\n",
        "\n",
        "combined_data = np.concatenate([test_data, cv_data])\n",
        "combined_labels = np.concatenate([test_labels, cv_labels])\n",
        "\n",
        "#np.random.shuffle(combined_data)\n",
        "indices = np.random.permutation(len(combined_data))\n",
        "combined_data = combined_data[indices]\n",
        "combined_labels = combined_labels[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp15wCpV-bK3"
      },
      "outputs": [],
      "source": [
        "val_split = int(len(combined_data) * 0.5)\n",
        "val_dataset = create_numpy_dataset(\n",
        "    combined_data[:val_split],\n",
        "    combined_labels[:val_split],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "test_dataset = create_numpy_dataset(\n",
        "    combined_data[val_split:],\n",
        "    combined_labels[val_split:],\n",
        "    BATCH_SIZE,\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "class_names = ['Negative', 'Benign Calcification', 'Benign Mass',\n",
        "               'Malignant Calcification', 'Malignant Mass']\n",
        "\n",
        "print(train_dataset.take(1))\n",
        "print(val_dataset.take(2))\n",
        "\n",
        "def print_label_distribution(labels):\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    dist = dict(zip(unique, counts))\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    for label_idx, count in dist.items():\n",
        "        print(f\"{class_names[label_idx]}: {count} samples ({count/len(labels)*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nValidation set:\")\n",
        "print_label_distribution(combined_labels[:val_split])\n",
        "print(\"\\nTest set:\")\n",
        "print_label_distribution(combined_labels[val_split:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xts9e0H4aSkR"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(train_dataset, strategy='advanced'):\n",
        "    \"\"\"\n",
        "    Calculate class weights with advanced boosting strategies for minority classes\n",
        "\n",
        "    Parameters:\n",
        "    train_dataset: TensorFlow dataset\n",
        "    \"\"\"\n",
        "    class_counts = np.zeros(5)\n",
        "    total_samples = 0\n",
        "\n",
        "\n",
        "    for _, labels in train_dataset:\n",
        "        batch_labels = labels.numpy()\n",
        "        class_counts += np.sum(batch_labels, axis=0)\n",
        "        total_samples += len(batch_labels)\n",
        "\n",
        "    # Calculate base frequencies\n",
        "    epsilon = 1e-7\n",
        "    class_frequencies = class_counts / total_samples\n",
        "\n",
        "    if strategy == 'basic':\n",
        "        # Original inverse frequency weighting\n",
        "        class_weights = 1 / (class_frequencies + epsilon)\n",
        "\n",
        "    elif strategy == 'custom':\n",
        "        class_weights = 1 / (class_frequencies + epsilon)\n",
        "\n",
        "        boost_factors = np.array([\n",
        "            1.0,\n",
        "            2.0,\n",
        "            1.8,\n",
        "            2.5,\n",
        "            2.0\n",
        "        ])\n",
        "\n",
        "        class_weights = class_weights * boost_factors\n",
        "\n",
        "    class_weights[class_counts == 0] = 0.0\n",
        "\n",
        "    # Normalize weights\n",
        "    if np.sum(class_weights) > 0:\n",
        "        class_weights = class_weights * len(class_counts) / np.sum(class_weights)\n",
        "\n",
        "        # Ensure minimum weight is 1.0\n",
        "        min_weight = np.min(class_weights[class_weights > 0])\n",
        "        class_weights = class_weights / min_weight\n",
        "\n",
        "    class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(f\"Total samples: {total_samples}\")\n",
        "    print(\"\\nClass Distribution:\")\n",
        "    label_names = ['Normal', 'Benign Calc', 'Benign Mass',\n",
        "                   'Malignant Calc', 'Malignant Mass']\n",
        "\n",
        "    print(f\"{'Class':<15} {'Count':>8} {'Frequency':>12} {'Weight':>10}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    for i, (count, freq, weight) in enumerate(zip(class_counts,\n",
        "                                                class_frequencies,\n",
        "                                                class_weights)):\n",
        "        print(f\"{label_names[i]:<15} {int(count):8d} {freq:12.4f} {weight:10.4f}\")\n",
        "\n",
        "    print(\"\\nWeight Statistics:\")\n",
        "    print(f\"Mean weight: {np.mean(class_weights):.4f}\")\n",
        "    print(f\"Max/Min ratio: {np.max(class_weights)/np.min(class_weights[class_weights>0]):.4f}\")\n",
        "\n",
        "    return class_weights_dict\n",
        "\n",
        "class_weights = calculate_class_weights(train_dataset, strategy='custom')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWplI2N6aSkR"
      },
      "outputs": [],
      "source": [
        "def create_custom_efficientnet(img_size=IMG_SIZE, num_classes=5):\n",
        "\n",
        "    # Load the base EfficientNetB0 model without top layers\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(img_size, img_size, 3)\n",
        "    )\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(img_size, img_size, 1))\n",
        "    x = tf.keras.layers.Conv2D(3, (1, 1), padding='same')(inputs)\n",
        "\n",
        "    x = base_model(x)\n",
        "\n",
        "    # Custom top layers with similar structure to your original model\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # First dense block\n",
        "    x = tf.keras.layers.Dense(512, activation='leaky_relu',\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "\n",
        "    # Second dense block\n",
        "    x = tf.keras.layers.Dense(256, activation='leaky_relu',\n",
        "                            kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mus0rVQsaSkS"
      },
      "outputs": [],
      "source": [
        "def compile_and_prepare_model(model):\n",
        "\n",
        "    initial_learning_rate = 5e-4\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.9\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        decay_rate=decay_rate,\n",
        "        staircase=True\n",
        "    )\n",
        "\n",
        "    # Optimizer with weight decay\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=initial_learning_rate,\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.Precision(),\n",
        "            tf.keras.metrics.Recall(),\n",
        "            tf.keras.metrics.AUC()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njHrLalAaSkS"
      },
      "outputs": [],
      "source": [
        "model = create_custom_efficientnet()\n",
        "model = compile_and_prepare_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t28MEJfRaSkS"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRXIxEVOaSkS"
      },
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    min_delta=1e-7,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.2,\n",
        "    patience=5,\n",
        "    min_delta=1e-7,\n",
        "    cooldown=0,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_model.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o7Tl3hdaSkS"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "                    epochs = EPOCHS,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    class_weight=class_weights,\n",
        "                    callbacks=[early_stopping, plateau, model_checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C54bOV1aSkS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "results = model.evaluate(test_dataset)\n",
        "print(\"\\nTest Results:\")\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "\n",
        "y_pred = np.argmax(model.predict(test_dataset), axis=1)\n",
        "y_true = np.concatenate([np.argmax(labels, axis=1)\n",
        "                        for _, labels in test_dataset])\n",
        "\n",
        "\n",
        "class_names = ['Normal', 'Benign Calc', 'Benign Mass',\n",
        "               'Malignant Calc', 'Malignant Mass']\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJBQfRr1NiY"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Normal/Abnormal Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OxYeCoMkfO-"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import tensorflow as tf\n",
        "\n",
        "# Clear GPU memory before training\n",
        "gc.collect()\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNtw8oCtaSkS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}